out_dir: 'v1_small_CHILI-3K_lora_r2'
eval_interval: 250  # how often to evaluate against the validation set
eval_iters_train: 200
eval_iters_val: 200
log_interval: 1  # how often to print to the console (1 = every iteration)
init_from: 'finetune'

device: 'cuda'
compile: True

dtype: 'float16'

# whether to always save a checkpoint
always_save_checkpoint: True

# whether to validate with a validation set
validate: True
validate_generation: False

dataset: 'datasets/CHILI-3K_nocond'
batch_size: 32
block_size: 1024

n_layer: 8
n_head: 8
n_embd: 512
dropout: 0.1

learning_rate: 5e-4
max_iters: 105_000
lr_decay_iters: 105_000
min_lr: 1e-4
beta2: 0.99

warmup_iters: 100

use_lora: True
lora_rank: 2
